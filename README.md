# **í‹ˆìƒˆ #2 ì™„ì „ ê°€ì´ë“œ: LOB ë°ì´í„° ì „ì²˜ë¦¬ vs ëª¨ë¸ ë³µì¡ë„**

---

## **I. ì—°êµ¬ ë°°ê²½ ë° ë™ê¸°**

### **í•µì‹¬ ë°œê²¬**
2025ë…„ 6ì›” ìµœì‹  ì—°êµ¬: cryptocurrency LOBì—ì„œ ë°ì´í„° ì „ì²˜ë¦¬ í›„ XGBoost ê°™ì€ ë‹¨ìˆœ ëª¨ë¸ì´ ë³µì¡í•œ neural networksë¥¼ 1-2% ìƒíšŒ. Binary/ternary ì˜ˆì¸¡ ì •í™•ë„ 0.42-0.71 (100ms-1000ms ì˜ˆì¸¡)

í•µì‹¬ ì£¼ì¥: "Better inputs matter more than stacking another hidden layer" - ëª¨ë¸ ë³µì¡ë„ë³´ë‹¤ input í’ˆì§ˆì´ ì¤‘ìš”

### **ì—°êµ¬ ê°­ (Research Gap)**
1. **ì²´ê³„ì  ë¹„êµ ë¶€ì¡±**: ì–´ë–¤ ì „ì²˜ë¦¬ê°€ ì–¸ì œ ì™œ íš¨ê³¼ì ì¸ì§€ ëª…í™•í•˜ì§€ ì•ŠìŒ
2. **ìì‚°ë³„ ì°¨ì´**: Liquid vs illiquid, crypto vs equity ë¹„êµ ë¶€ì¡±
3. **ì´ë¡ ì  ì„¤ëª… ë¶€ì¡±**: ì™œ ì „ì²˜ë¦¬ê°€ ë„ì›€ì´ ë˜ëŠ”ì§€ ë©”ì»¤ë‹ˆì¦˜ ë¶ˆëª…í™•
4. **í•œêµ­ ì‹œì¥ ì—°êµ¬ ì „ë¬´**: KOSPI/KOSDAQ LOB ë°ì´í„° ì—°êµ¬ ì—†ìŒ
5. **ì‹¤ë¬´ ê°€ì´ë“œë¼ì¸ ë¶€ì¬**: Practitionersë¥¼ ìœ„í•œ ì²´ê³„ì  ì§€ì¹¨ ì—†ìŒ

---

## **II. ì—°êµ¬ ì§ˆë¬¸ (Research Questions)**

### **Main RQ:**
**"ì–´ë–¤ preprocessing ë°©ë²•ì´ ì–´ë–¤ ì¡°ê±´ì—ì„œ LOB mid-price ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ”ê°€?"**

### **Sub-RQs:**
1. **RQ1**: Savitzky-Golay vs Kalman vs Wavelet - ì–´ëŠ ê²ƒì´ ì–¸ì œ ìš°ìˆ˜í•œê°€?
2. **RQ2**: Liquid stocks vs illiquid stocks - ì „ì²˜ë¦¬ íš¨ê³¼ê°€ ë‹¤ë¥¸ê°€?
3. **RQ3**: ë‹¨ìˆœ ëª¨ë¸ vs ë³µì¡ ëª¨ë¸ - ì „ì²˜ë¦¬ê°€ ì–´ë””ì— ë” ë„ì›€ì´ ë˜ëŠ”ê°€?
4. **RQ4**: ì˜ˆì¸¡ horizon (100ms vs 1s vs 10s) - ì „ì²˜ë¦¬ íš¨ê³¼ê°€ ë‹¬ë¼ì§€ëŠ”ê°€?
5. **RQ5**: **WHY** - Signal-to-noise ratio ê°œì„ ì´ ë©”ì»¤ë‹ˆì¦˜ì¸ê°€?

---

## **III. ë°©ë²•ë¡  (Methodology)**

### **A. ë°ì´í„°**

**1. Cryptocurrency (Primary)**
- **Source**: Bybit public historical data
- **Assets**: BTC/USDT, ETH/USDT
- **Period**: 1-3ê°œì›”
- **Frequency**: 100ms snapshots
- **Depth**: 5, 10, 20, 40 levels
- **ì¥ì **: ë¬´ë£Œ, 24/7 ê±°ë˜, high-frequency

**2. í•œêµ­ ì£¼ì‹ (Secondary, if possible)**
- **Assets**: 
  - Liquid: ì‚¼ì„±ì „ì (005930)
  - Illiquid: KOSDAQ ì¤‘ì†Œí˜•ì£¼ 1ì¢…
- **Source**: 
  - KOSCOM (ìœ ë£Œ)
  - ë˜ëŠ” í¬ë˜í”„í†¤/ì¦ê¶Œì‚¬ API
- **Challenge**: ë°ì´í„° ì ‘ê·¼ì„±

**3. ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°**
- **NASDAQ**: INTC (LOBSTER) - ê¸°ì¡´ ì—°êµ¬ì™€ ë¹„êµìš©

---

### **B. ì „ì²˜ë¦¬ ë°©ë²• (Preprocessing Methods)**

**1. Savitzky-Golay Filter**
```python
from scipy.signal import savgol_filter
# Window size: 5, 11, 21
# Polynomial order: 2, 3
filtered = savgol_filter(mid_price, window_length=11, polyorder=2)
```
- **íŠ¹ì§•**: Local polynomial regression
- **ì¥ì **: Edge-preserving, ë¹ ë¦„
- **ë‹¨ì **: Window size ì„ íƒ ë¯¼ê°

**2. Kalman Filter**
```python
from pykalman import KalmanFilter
kf = KalmanFilter(initial_state_mean=0, n_dim_obs=1)
filtered_state_means, _ = kf.filter(mid_price)
```
- **íŠ¹ì§•**: Recursive Bayesian estimation
- **ì¥ì **: Real-time ì ìš© ê°€ëŠ¥, ì´ë¡ ì  ê¸°ë°˜
- **ë‹¨ì **: Parameter tuning í•„ìš”

**3. Wavelet Denoising**
```python
import pywt
coeffs = pywt.wavedec(mid_price, 'db4', level=3)
# Soft thresholding
filtered = pywt.waverec(coeffs_thresh, 'db4')
```
- **íŠ¹ì§•**: Multi-resolution analysis
- **ì¥ì **: Frequency-domain filtering
- **ë‹¨ì **: Computationally expensive

**4. Moving Average (Baseline)**
```python
# Simple MA, Exponential MA
ma = mid_price.rolling(window=10).mean()
ema = mid_price.ewm(span=10).mean()
```

**5. Raw Data (Control)**
- No preprocessing

---

### **C. Feature Engineering**

**ê¸°ë³¸ Features (Existing literature):**
1. **Price features**:
   - Mid-price: $(P_{ask}^1 + P_{bid}^1) / 2$
   - Spread: $P_{ask}^1 - P_{bid}^1$
   - Microprice: Volume-weighted

2. **Volume features**:
   - Order imbalance: $\frac{V_{bid} - V_{ask}}{V_{bid} + V_{ask}}$
   - Total volume at each level

3. **Order Flow Imbalance (OFI)**:
   $OFI = \sum_{levels} \Delta(volume \times price)$

4. **Time features**:
   - Time since last trade
   - Volatility (rolling std)

**Your contribution: ì „ì²˜ë¦¬ ì ìš© í›„ features**

---

### **D. Models**

**Simple Models:**
1. **Logistic Regression** (Baseline)
2. **XGBoost**
   ```python
   xgb.XGBClassifier(
       max_depth=3,
       learning_rate=0.1,
       n_estimators=100
   )
   ```
3. **CatBoost**

**Deep Learning Models:**
4. **Simple CNN**
   ```python
   Conv1D(64, 3) -> ReLU -> BatchNorm
   Conv1D(64, 3) -> ReLU -> BatchNorm
   GlobalMaxPooling -> Dense(32) -> Output
   ```

5. **DeepLOB** (Benchmark)
   Zhang et al. 2019 architecture - ê¸°ì¡´ ì—°êµ¬ í‘œì¤€

6. **Conv1D + LSTM**

---

### **E. ì‹¤í—˜ ì„¤ê³„**

**Prediction Tasks:**
- **Binary**: Up/Down (mid-price change > 0)
- **Ternary**: Up/Flat/Down (threshold Î¸ = 1 tick)

**Prediction Horizons:**
- T = 100ms, 500ms, 1s, 5s, 10s

**Train/Val/Test Split:**
- Train: 60%
- Validation: 20%
- Test: 20%
- **ì¤‘ìš”**: Temporal split (no look-ahead bias)

**Ablation Study:**
| Experiment | Preprocessing | Model | LOB Depth |
|------------|--------------|-------|-----------|
| Exp1 | None (raw) | All models | 5/10/20/40 |
| Exp2 | Savitzky-Golay | All models | 5/10/20/40 |
| Exp3 | Kalman | All models | 5/10/20/40 |
| Exp4 | Wavelet | All models | 5/10/20/40 |
| Exp5 | Moving Average | All models | 5/10/20/40 |

**Total experiments**: 5 preprocessing Ã— 6 models Ã— 4 depths Ã— 5 horizons = **600 configurations**

---

### **F. í‰ê°€ ì§€í‘œ**

**Prediction Performance:**
1. **Accuracy**
2. **F1-Score** (class imbalance ê³ ë ¤)
3. **Matthews Correlation Coefficient (MCC)**
4. **Confusion Matrix**

**Signal Quality:**
5. **Signal-to-Noise Ratio (SNR)**:
   $$SNR = 10 \log_{10}\left(\frac{\sigma_{signal}^2}{\sigma_{noise}^2}\right)$$

6. **Autocorrelation** (before/after preprocessing)

**Computational Efficiency:**
7. **Training Time**
8. **Inference Latency** (ms per prediction)
9. **Memory Usage**

**Trading Simulation (Optional, if time permits):**
10. **Sharpe Ratio**
11. **Maximum Drawdown**
12. **Total Return**

---

## **IV. ì˜ˆìƒ ê²°ê³¼ (Expected Findings)**

### **Hypothesis 1: ì „ì²˜ë¦¬ íš¨ê³¼ëŠ” ìì‚° ìœ ë™ì„±ì— ë”°ë¼ ë‹¤ë¥´ë‹¤**
- **Illiquid assets**: ì „ì²˜ë¦¬ íš¨ê³¼ í¼ (noise ë§ìŒ)
- **Liquid assets**: ì „ì²˜ë¦¬ íš¨ê³¼ ì‘ìŒ (signal ì´ë¯¸ ê°•í•¨)

### **Hypothesis 2: ë‹¨ìˆœ ëª¨ë¸ì´ ì „ì²˜ë¦¬ë¡œ ë” ë§ì´ ê°œì„ ëœë‹¤**
- XGBoost + preprocessing > DeepLOB (raw)
- Deep modelsëŠ” ë‚´ë¶€ì ìœ¼ë¡œ denoising í•™ìŠµ

### **Hypothesis 3: Savitzky-Golayê°€ ê°€ì¥ practical**
- Latency vs accuracy trade-offì—ì„œ ìµœì 
- Kalmanì€ ì´ë¡ ì ìœ¼ë¡œ ìš°ìˆ˜í•˜ì§€ë§Œ ëŠë¦¼

### **Hypothesis 4: Short horizonì—ì„œ ì „ì²˜ë¦¬ íš¨ê³¼ í¼**
- T=100ms: ì „ì²˜ë¦¬ í•„ìˆ˜
- T=10s: ì „ì²˜ë¦¬ íš¨ê³¼ ê°ì†Œ (ì¥ê¸° íŠ¸ë Œë“œê°€ noise ì••ë„)

---

## **V. ê¸°ì—¬ë„ (Contributions)**

### **í•™ìˆ ì  ê¸°ì—¬:**
1. âœ… **ì²« ì²´ê³„ì  LOB ì „ì²˜ë¦¬ ë¹„êµ ì—°êµ¬**
2. âœ… **ì´ë¡ ì  ë©”ì»¤ë‹ˆì¦˜ ì„¤ëª…** (SNR ë¶„ì„)
3. âœ… **ìì‚°ë³„/horizonë³„ ì°¨ì´ ê·œëª…**
4. âœ… **í•œêµ­ ì‹œì¥ ì²« LOB microstructure ì—°êµ¬**

### **ì‹¤ë¬´ì  ê¸°ì—¬:**
5. âœ… **Practitioner guideline**: "ì–¸ì œ ì–´ë–¤ ì „ì²˜ë¦¬ë¥¼ ì“¸ ê²ƒì¸ê°€"
6. âœ… **ì˜¤í”ˆì†ŒìŠ¤ ì½”ë“œ** (ì¬í˜„ ê°€ëŠ¥ì„±)
7. âœ… **Latency-aware recommendations**

### **ê¸°ìˆ ì  ê¸°ì—¬:**
8. âœ… **Efficient pipeline** (LOBFrame í™•ì¥)
9. âœ… **Comparative framework** for future research

---

## **VI. ì¶œíŒ ì „ëµ (Publication Strategy)**

### **Target 1: êµ­ë‚´ í•™íšŒ (í™•ì‹¤)**
- **í•œêµ­ê¸ˆìœµê³µí•™íšŒ í•™ìˆ ëŒ€íšŒ** (ì—° 2íšŒ)
- **í•œêµ­ê²½ì˜ê³¼í•™íšŒ ì¶”ê³„í•™ìˆ ëŒ€íšŒ**
- **í•œêµ­ë°ì´í„°ì •ë³´ê³¼í•™íšŒ**
- **ì¥ì **: ë¹ ë¥¸ í”¼ë“œë°±, í•œêµ­ì–´ ë°œí‘œ ê°€ëŠ¥, ë„¤íŠ¸ì›Œí‚¹

### **Target 2: International Conference**
- **ICAIF (ACM Int'l Conf on AI in Finance)** - Top tier
- **KDD Workshop on Financial Data Science**
- **NeurIPS Workshop on ML in Finance**

### **Target 3: Journal (SCI/SSCI)**
- **Tier 1 (ë„ì „)**:
  - Journal of Computational Finance
  - Quantitative Finance
  
- **Tier 2 (í˜„ì‹¤ì )**:
  - Expert Systems with Applications (SCI)
  - Applied Soft Computing (SCI)
  - Finance Research Letters (SSCI)
  
- **Tier 3 (ì•ˆì „)**:
  - í•œêµ­ê²½ì˜ê³¼í•™íšŒì§€ (KCI)
  - ì¬ë¬´ì—°êµ¬ (KCI)

---

## **VII. íƒ€ì„ë¼ì¸ (12ì£¼ ê³„íš)**

### **Week 1-2: ì¤€ë¹„ ë‹¨ê³„**
- [ ] Literature review ì™„ë£Œ
- [ ] ë°ì´í„° ë‹¤ìš´ë¡œë“œ (Bybit)
- [ ] í™˜ê²½ ì„¤ì • (Python, GPU)
- [ ] LOBFrame ì„¤ì¹˜ ë° í…ŒìŠ¤íŠ¸

### **Week 3-4: ë°ì´í„° ì „ì²˜ë¦¬**
- [ ] Raw LOB ë°ì´í„° íŒŒì‹±
- [ ] 5ê°€ì§€ ì „ì²˜ë¦¬ êµ¬í˜„
- [ ] Feature extraction
- [ ] Train/val/test split

### **Week 5-6: ëª¨ë¸ êµ¬í˜„**
- [ ] 6ê°œ ëª¨ë¸ êµ¬í˜„
- [ ] Hyperparameter tuning (validation set)
- [ ] Baseline ê²°ê³¼ í™•ë³´

### **Week 7-8: ì‹¤í—˜ ì‹¤í–‰**
- [ ] 600 configurations ì‹¤í—˜
- [ ] ê²°ê³¼ ë¡œê¹… (MLflow/WandB)
- [ ] Intermediate ë¶„ì„

### **Week 9-10: ë¶„ì„ ë° í•´ì„**
- [ ] SNR ë¶„ì„
- [ ] Statistical significance tests
- [ ] Ablation study ê²°ê³¼ ì •ë¦¬
- [ ] Visualization (heatmaps, confusion matrices)

### **Week 11: ë…¼ë¬¸ ì‘ì„±**
- [ ] Introduction
- [ ] Methodology
- [ ] Results
- [ ] Discussion
- [ ] Conclusion

### **Week 12: ìµœì¢… ì ê²€**
- [ ] êµìˆ˜ í”¼ë“œë°± ë°˜ì˜
- [ ] ì½”ë“œ ì •ë¦¬ (GitHub)
- [ ] Supplementary materials
- [ ] ìµœì¢… ì œì¶œ

---

## **VIII. êµ¬í˜„ ìƒì„¸ (Implementation Details)**

### **A. ì½”ë“œ êµ¬ì¡°**
```
lob_preprocessing/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ download.py          # Bybit ë°ì´í„° ë‹¤ìš´ë¡œë“œ
â”‚   â”œâ”€â”€ preprocess.py         # ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤
â”‚   â””â”€â”€ features.py           # Feature engineering
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ baseline.py           # Logistic, XGBoost
â”‚   â”œâ”€â”€ deep_models.py        # CNN, LSTM, DeepLOB
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_experiments.py    # ì‹¤í—˜ ì‹¤í–‰
â”‚   â”œâ”€â”€ configs.yaml          # ì„¤ì • íŒŒì¼
â”‚   â””â”€â”€ evaluate.py           # í‰ê°€
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ snr_analysis.py       # SNR ê³„ì‚°
â”‚   â”œâ”€â”€ statistical_tests.py  # t-test, etc.
â”‚   â””â”€â”€ visualize.py          # Plots
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ EDA.ipynb
â”‚   â””â”€â”€ Results.ipynb
â””â”€â”€ requirements.txt
```

### **B. í•µì‹¬ ì „ì²˜ë¦¬ ì½”ë“œ**

```python
class LOBPreprocessor:
    def __init__(self, method='savgol'):
        self.method = method
        
    def denoise(self, mid_prices):
        if self.method == 'savgol':
            return savgol_filter(mid_prices, 11, 2)
        elif self.method == 'kalman':
            kf = KalmanFilter(...)
            return kf.filter(mid_prices)[0]
        elif self.method == 'wavelet':
            return self._wavelet_denoise(mid_prices)
        elif self.method == 'ma':
            return mid_prices.rolling(10).mean()
        else:  # raw
            return mid_prices
    
    def compute_snr(self, original, filtered):
        signal = filtered
        noise = original - filtered
        return 10 * np.log10(np.var(signal) / np.var(noise))
```

### **C. ì‹¤í—˜ ì‹¤í–‰ ì˜ˆì‹œ**

```python
# experiments/run_experiments.py
import itertools

preprocessors = ['raw', 'savgol', 'kalman', 'wavelet', 'ma']
models = ['logistic', 'xgboost', 'catboost', 'cnn', 'deeplob', 'lstm']
depths = [5, 10, 20, 40]
horizons = [100, 500, 1000, 5000, 10000]  # ms

for prep, model, depth, horizon in itertools.product(...):
    # Load data
    data = load_lob_data(depth=depth)
    
    # Preprocess
    preprocessor = LOBPreprocessor(method=prep)
    data_preprocessed = preprocessor.denoise(data)
    
    # Extract features
    features = extract_features(data_preprocessed)
    
    # Train model
    clf = get_model(model)
    clf.fit(X_train, y_train)
    
    # Evaluate
    results = evaluate(clf, X_test, y_test)
    
    # Log results
    log_to_mlflow(prep, model, depth, horizon, results)
```

---

## **IX. ì ì¬ì  ë¬¸ì œ ë° í•´ê²°ì±…**

### **Problem 1: ë°ì´í„° í¬ê¸°**
- **Issue**: LOB ë°ì´í„°ê°€ ë§¤ìš° í¼ (GB ë‹¨ìœ„)
- **Solution**: 
  - Chunk-wise processing
  - HDF5 í¬ë§· ì‚¬ìš©
  - ìƒ˜í”Œë§ (evenly-spaced)

### **Problem 2: Class Imbalance**
- **Issue**: Up/Down ë¹„ìœ¨ ë¶ˆê· í˜•
- **Solution**:
  - Stratified split
  - Class weights
  - SMOTE (ì‹ ì¤‘í•˜ê²Œ)

### **Problem 3: ê³„ì‚° ì‹œê°„**
- **Issue**: 600 configurations Ã— long training
- **Solution**:
  - GPU ì‚¬ìš©
  - Parallel processing (joblib)
  - Early stopping

### **Problem 4: í•œêµ­ ë°ì´í„° ì ‘ê·¼**
- **Issue**: KOSPI LOB ë°ì´í„° ìœ ë£Œ/ì œí•œì 
- **Solution**:
  - Cryptoì— ì§‘ì¤‘ (ì¶©ë¶„í•¨)
  - êµìˆ˜/í•™êµ ë¼ì´ì„ ìŠ¤ í™œìš©
  - ë˜ëŠ” "future work"ë¡œ ë‚¨ê¹€

---

## **X. ë…¼ë¬¸ êµ¬ì¡° (Outline)**

### **Abstract** (150-200 words)
- Background: LOB prediction ì¤‘ìš”ì„±
- Problem: ëª¨ë¸ ë³µì¡ë„ vs ë°ì´í„° í’ˆì§ˆ
- Method: ì²´ê³„ì  ì „ì²˜ë¦¬ ë¹„êµ
- Results: ì „ì²˜ë¦¬ê°€ 1-2% ê°œì„ , ë‹¨ìˆœ ëª¨ë¸ sufficient
- Contribution: ì‹¤ë¬´ ê°€ì´ë“œë¼ì¸

### **1. Introduction**
- 1.1 Motivation
- 1.2 Research Gap
- 1.3 Research Questions
- 1.4 Contributions
- 1.5 Paper Structure

### **2. Literature Review**
- 2.1 Limit Order Book Microstructure
- 2.2 Machine Learning for LOB Prediction
- 2.3 Data Preprocessing in Finance
- 2.4 Signal Processing Techniques

### **3. Methodology**
- 3.1 Data Description
- 3.2 Preprocessing Methods
- 3.3 Feature Engineering
- 3.4 Models
- 3.5 Experimental Design
- 3.6 Evaluation Metrics

### **4. Results**
- 4.1 Overall Performance Comparison
- 4.2 Effect of Preprocessing by Asset Type
- 4.3 Effect by Model Complexity
- 4.4 Effect by Prediction Horizon
- 4.5 Signal-to-Noise Ratio Analysis
- 4.6 Computational Efficiency

### **5. Discussion**
- 5.1 Interpretation of Findings
- 5.2 When to Use Which Preprocessing?
- 5.3 Practical Guidelines
- 5.4 Limitations

### **6. Conclusion**
- 6.1 Summary
- 6.2 Contributions
- 6.3 Future Work

### **References**

### **Appendices**
- A. Hyperparameter Settings
- B. Additional Results
- C. Code Availability

---

## **XI. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ (Key Insights - ë¯¸ë¦¬ ì¤€ë¹„)**

ë…¼ë¬¸ì—ì„œ ê°•ì¡°í•  ë©”ì‹œì§€:

1. **"Simple is Better (with good data)"**
   - XGBoost + preprocessing â‰¥ DeepLOB (raw)
   - ì‹¤ë¬´ì—ì„œëŠ” ë‹¨ìˆœ ëª¨ë¸ ì„ í˜¸ (í•´ì„ ê°€ëŠ¥, ë¹ ë¦„)

2. **"Know Your Data"**
   - Illiquid assets â†’ aggressive preprocessing
   - Liquid assets â†’ minimal preprocessing
   - ìì‚° íŠ¹ì„± íŒŒì•…ì´ ì„ ê²° ê³¼ì œ

3. **"One Size Doesn't Fit All"**
   - Horizonë³„ë¡œ ìµœì  ì „ì²˜ë¦¬ ë‹¤ë¦„
   - Short horizon: Savitzky-Golay
   - Long horizon: Raw data sufficient

4. **"Latency Matters"**
   - Wavelet: ì •í™•í•˜ì§€ë§Œ ëŠë¦¼ â†’ HFT ë¶€ì í•©
   - Savitzky-Golay: ë¹ ë¥´ê³  ì¶©ë¶„íˆ ì¢‹ìŒ â†’ ì‹¤ìš©ì 

5. **"Signal-to-Noise is the Mechanism"**
   - ì „ì²˜ë¦¬ íš¨ê³¼ëŠ” SNR ê°œì„ ìœ¼ë¡œ ì„¤ëª… ê°€ëŠ¥
   - ì´ë¡ ì  ê¸°ë°˜ ì œê³µ

---

## **XII. ë¦¬ìŠ¤í¬ ê´€ë¦¬**

### **High Risk Items:**
1. **í•œêµ­ ë°ì´í„° ì ‘ê·¼ ì‹¤íŒ¨**
   - **Mitigation**: Cryptoì—ë§Œ ì§‘ì¤‘ (ì¶©ë¶„í•¨)

2. **ê²°ê³¼ê°€ ê¸°ì¡´ ì—°êµ¬ì™€ ë‹¤ë¦„**
   - **Mitigation**: ì°¨ì´ ì„¤ëª…í•˜ëŠ” ì„¹ì…˜ ì¶”ê°€, ì¬í˜„ì„± ê°•ì¡°

3. **ê³„ì‚° ì‹œê°„ ì´ˆê³¼**
   - **Mitigation**: ì‹¤í—˜ ë²”ìœ„ ì¶•ì†Œ (depth 2ê°œë§Œ, horizon 3ê°œë§Œ)

### **Medium Risk Items:**
4. **êµìˆ˜ ë°˜ì‘ ë¶€ì •ì **
   - **Mitigation**: ì¤‘ê°„ ë°œí‘œë¡œ í”¼ë“œë°± ì¡°ê¸° í™•ë³´

5. **ì €ë„ ë¦¬ì **
   - **Mitigation**: êµ­ë‚´ í•™íšŒ ë¨¼ì €, í”¼ë“œë°± ë°˜ì˜ í›„ ì €ë„

---

## **XIII. ì„±ê³µ ê¸°ì¤€**

### **Minimum Viable Thesis (ìµœì†Œ ìš”ê±´):**
- âœ… Crypto LOB (BTC) 1ê°œ ìì‚°
- âœ… 3ê°œ ì „ì²˜ë¦¬ ë°©ë²•
- âœ… 4ê°œ ëª¨ë¸
- âœ… ëª…í™•í•œ ê²°ê³¼ (preprocessing helps)
- âœ… ì¡¸ì—… í†µê³¼

### **Good Thesis (ëª©í‘œ):**
- âœ… Crypto 2ê°œ ìì‚° (BTC, ETH)
- âœ… 5ê°œ ì „ì²˜ë¦¬ ë°©ë²•
- âœ… 6ê°œ ëª¨ë¸
- âœ… SNR ë¶„ì„ í¬í•¨
- âœ… êµ­ë‚´ í•™íšŒ ë°œí‘œ

### **Excellent Thesis (ì´ìƒì ):**
- âœ… Crypto + í•œêµ­ ì£¼ì‹
- âœ… ì™„ì „í•œ ablation study
- âœ… Trading simulation
- âœ… ì˜¤í”ˆì†ŒìŠ¤ ì½”ë“œ
- âœ… International conference acceptance

---

## **XIV. ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸**

### **ì‹œì‘ ì „ (Week 0):**
- [ ] êµìˆ˜ ìŠ¹ì¸ í™•ë³´
- [ ] GPU ì„œë²„ ì ‘ê·¼ í™•ì¸
- [ ] ë°ì´í„° ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸
- [ ] í™˜ê²½ ì„¤ì • ì™„ë£Œ

### **ì¤‘ê°„ ì ê²€ (Week 6):**
- [ ] Baseline ê²°ê³¼ í™•ë³´
- [ ] ì¤‘ê°„ ë°œí‘œ ì¤€ë¹„
- [ ] êµìˆ˜ í”¼ë“œë°± ë°›ê¸°
- [ ] ì¼ì • ì¬ì¡°ì •

### **ë§ˆë¬´ë¦¬ (Week 12):**
- [ ] ë…¼ë¬¸ ì´ˆì•ˆ ì™„ì„±
- [ ] ì½”ë“œ ì •ë¦¬ ë° ë¬¸ì„œí™”
- [ ] ë°œí‘œ ìë£Œ ì¤€ë¹„
- [ ] ìµœì¢… ì œì¶œ

---

## **XV. ì—°ë½ì²˜ ë° ìë£Œ**

### **ë°ì´í„° ì†ŒìŠ¤:**
- Bybit: https://www.bybit.com/derivatives/en/history-data
- LOBSTER: https://lobsterdata.com/
- LOBFrame: https://github.com/...

### **ì°¸ê³  ì½”ë“œ:**
- DeepLOB: https://github.com/zcakhaa/DeepLOB-Deep-Convolutional-Neural-Networks-for-Limit-Order-Books
- XGBoost: https://xgboost.readthedocs.io/

### **ê´€ë ¨ ë…¼ë¬¸:**
- Exploring Microstructural Dynamics (2025)
- Deep Limit Order Book Forecasting (2024)
- Feature Engineering for Mid-Price Prediction (2019)

---

# **ì™„ë£Œ. ì´ì œ ì‹œì‘í•˜ì„¸ìš”!**

**ì§ˆë¬¸ì´ë‚˜ ë§‰íˆëŠ” ë¶€ë¶„ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”. ë‹¹ì‹ ì€ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í™”ì´íŒ…!** ğŸ”¥