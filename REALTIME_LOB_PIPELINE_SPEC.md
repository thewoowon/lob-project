# LOB ì‹¤ì‹œê°„ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ëª…ì„¸ì„œ

**ë²„ì „**: 1.0
**ì‘ì„±ì¼**: 2025-12-29
**ê¸°ì¤€ ë¬¸ì„œ**: PAPER_DRAFT.md

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#1-ê°œìš”)
2. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#2-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
3. [ë°ì´í„° ìˆ˜ì§‘ (ì™„ë£Œ)](#3-ë°ì´í„°-ìˆ˜ì§‘-ì™„ë£Œ)
4. [íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸](#4-íŠ¹ì§•-ì—”ì§€ë‹ˆì–´ë§-íŒŒì´í”„ë¼ì¸)
5. [ëª¨ë¸ í•™ìŠµ ë° í‰ê°€](#5-ëª¨ë¸-í•™ìŠµ-ë°-í‰ê°€)
6. [ì‹¤ì‹œê°„ ì¶”ë¡  ì‹œìŠ¤í…œ](#6-ì‹¤ì‹œê°„-ì¶”ë¡ -ì‹œìŠ¤í…œ)
7. [ì„±ëŠ¥ ëª©í‘œ ë° ê²€ì¦](#7-ì„±ëŠ¥-ëª©í‘œ-ë°-ê²€ì¦)
8. [êµ¬í˜„ ë‹¨ê³„](#8-êµ¬í˜„-ë‹¨ê³„)

---

## 1. ê°œìš”

### 1.1 í”„ë¡œì íŠ¸ ëª©ì 

í•œêµ­ ì£¼ì‹ì‹œì¥ LOB (Limit Order Book) ë°ì´í„°ë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„ ì¤‘ê°„ê°€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ êµ¬ì¶•:
- **Raw LOB features (40ê°œ)** + **Engineered features (38ê°œ)** = **78 features** í™œìš©
- **CatBoost** ê¸°ë°˜ 3-class ë¶„ë¥˜ (í•˜ë½/ë³´í•©/ìƒìŠ¹)
- **ëª©í‘œ ì •í™•ë„**: 73.43% Â± 0.33% (PAPER_DRAFT.md ê¸°ì¤€)

### 1.2 í•µì‹¬ ì›ì¹™

**PAPER_DRAFT.mdì˜ í•µì‹¬ ë°œê²¬ ì ìš©**:
1. âœ… **Raw + Engineered ì¡°í•© í•„ìˆ˜**: Rawë§Œ 68.47%, Engineeredë§Œ 63.14%, ì¡°í•© ì‹œ 73.43%
2. âœ… **ì „ì²˜ë¦¬ ë¶ˆí•„ìš”**: ì´ë¯¸ ì •ê·œí™”ëœ ë°ì´í„°ì—ì„œ ì „ì²˜ë¦¬ëŠ” íš¨ê³¼ ì—†ìŒ (+0.64pp)
3. âœ… **í†µê³„ì  ê²€ì¦**: 3-5 random seeds, p-value < 0.001
4. âœ… **ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€**: Temporal split, ë¯¸ë˜ ì •ë³´ ì‚¬ìš© ê¸ˆì§€

### 1.3 ê¸°ìˆ  ìŠ¤íƒ

```
Data Collection:  EC2 t4g.nano (ARM64) + S3 JSONL storage
Feature Engineering: Python 3.9 + NumPy 1.23.5 + Pandas 1.5.3
Model Training:   CatBoost 1.2
Real-time Inference: Python + WebSocket
Validation:       scikit-learn 1.2.2
```

---

## 2. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 2.1 ì „ì²´ êµ¬ì„±ë„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LOB ì‹¤ì‹œê°„ ì¶”ë¡  íŒŒì´í”„ë¼ì¸                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Data        â”‚      â”‚  2. Feature      â”‚      â”‚  3. Model   â”‚
â”‚  Collection     â”‚â”€â”€â”€â”€â”€â–¶â”‚  Engineering     â”‚â”€â”€â”€â”€â”€â–¶â”‚  Training   â”‚
â”‚  (EC2 + S3)     â”‚      â”‚  (38 features)   â”‚      â”‚  (CatBoost) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    âœ… DONE                  ğŸ”¨ TODO                  ğŸ”¨ TODO
                                   â”‚
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Real-time Inference Pipeline                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ WebSocket    â”‚â”€â”€â–¶â”‚ Feature      â”‚â”€â”€â–¶â”‚ Model        â”‚        â”‚
â”‚  â”‚ LOB Stream   â”‚   â”‚ Computation  â”‚   â”‚ Prediction   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            ğŸ”¨ TODO
```

### 2.2 ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
lob-project/
â”œâ”€â”€ data_collection/              # âœ… DONE
â”‚   â”œâ”€â”€ ec2_setup/
â”‚   â”‚   â”œâ”€â”€ kis_lob_collector_ec2.py
â”‚   â”‚   â””â”€â”€ kis_lob_service.service
â”‚   â””â”€â”€ s3_data/                  # S3ì— ì €ì¥ëœ JSONL íŒŒì¼ë“¤
â”‚
â”œâ”€â”€ feature_engineering/          # ğŸ”¨ Phase 1
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ raw_features.py           # Raw 40 features ì¶”ì¶œ
â”‚   â”œâ”€â”€ engineered_features.py    # 38 features ê³„ì‚°
â”‚   â”‚   â”œâ”€â”€ price_features.py     # 6 features
â”‚   â”‚   â”œâ”€â”€ volume_features.py    # 8 features
â”‚   â”‚   â”œâ”€â”€ order_imbalance.py    # 6 features (OI)
â”‚   â”‚   â”œâ”€â”€ order_flow_imbalance.py # 6 features (OFI)
â”‚   â”‚   â”œâ”€â”€ depth_features.py     # 6 features
â”‚   â”‚   â””â”€â”€ price_impact.py       # 6 features
â”‚   â”œâ”€â”€ pipeline.py               # ì „ì²´ 78 features ìƒì„±
â”‚   â””â”€â”€ utils.py                  # ê³µí†µ ìœ í‹¸ë¦¬í‹°
â”‚
â”œâ”€â”€ model_training/               # ğŸ”¨ Phase 2
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train_catboost.py         # CatBoost í•™ìŠµ
â”‚   â”œâ”€â”€ evaluate.py               # 5-seed validation
â”‚   â”œâ”€â”€ hyperparameter_tuning.py  # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
â”‚   â””â”€â”€ data_leakage_check.py     # ë°ì´í„° ëˆ„ìˆ˜ ê²€ì¦
â”‚
â”œâ”€â”€ realtime_inference/           # ğŸ”¨ Phase 3
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ websocket_client.py       # LOB ìŠ¤íŠ¸ë¦¼ ìˆ˜ì‹ 
â”‚   â”œâ”€â”€ feature_computer.py       # ì‹¤ì‹œê°„ feature ê³„ì‚°
â”‚   â”œâ”€â”€ predictor.py              # ëª¨ë¸ ì¶”ë¡ 
â”‚   â””â”€â”€ buffer.py                 # ê³¼ê±° ë°ì´í„° ë²„í¼ ê´€ë¦¬
â”‚
â”œâ”€â”€ experiments/                  # ğŸ”¨ Phase 4 (ì„ íƒ)
â”‚   â”œâ”€â”€ ablation_study.py         # Feature group ì¤‘ìš”ë„ ë¶„ì„
â”‚   â”œâ”€â”€ random_baseline.py        # Random feature baseline
â”‚   â””â”€â”€ translob_comparison.py    # TransLOB ë¹„êµ
â”‚
â”œâ”€â”€ validation/
â”‚   â”œâ”€â”€ statistical_tests.py      # t-test, p-value ê³„ì‚°
â”‚   â”œâ”€â”€ temporal_split_check.py   # Train/test split ê²€ì¦
â”‚   â””â”€â”€ causality_check.py        # Feature causality ê²€ì¦
â”‚
â”œâ”€â”€ models/                       # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
â”‚   â””â”€â”€ catboost_seed_{seed}.cbm
â”‚
â”œâ”€â”€ results/                      # ì‹¤í—˜ ê²°ê³¼
â”‚   â”œâ”€â”€ metrics.json
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â””â”€â”€ feature_importance.csv
â”‚
â””â”€â”€ configs/
    â”œâ”€â”€ feature_config.yaml       # Feature ê³„ì‚° ì„¤ì •
    â”œâ”€â”€ model_config.yaml         # ëª¨ë¸ í•™ìŠµ ì„¤ì •
    â””â”€â”€ inference_config.yaml     # ì‹¤ì‹œê°„ ì¶”ë¡  ì„¤ì •
```

---

## 3. ë°ì´í„° ìˆ˜ì§‘ (ì™„ë£Œ)

### 3.1 í˜„ì¬ ìƒíƒœ âœ…

**ì¸í”„ë¼**: EC2 t4g.nano (us-east-1)
**ë°ì´í„° í˜•ì‹**: JSONL (S3 ì €ì¥)
**ìˆ˜ì§‘ ì¢…ëª©**: 10ê°œ (005930 ì‚¼ì„±ì „ì ë“±)
**ìˆ˜ì§‘ ê¸°ê°„**: 2ê°œì›” (ìë™ ìˆ˜ì§‘ ì¤‘)
**ë°ì´í„° ë¹ˆë„**: ì¥ì¤‘ ì‹¤ì‹œê°„ (09:00-15:30 KST)

### 3.2 ë°ì´í„° í˜•ì‹

**ì›ë³¸ LOB ë°ì´í„°** (KIS API íŒŒì´í”„ êµ¬ë¶„ í˜•ì‹):
```
0|H0STASP0|001|005930^123607^0^105100^105200^...^64675^68489^...
                â”œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬
                â”‚     â”‚     â”‚  â”‚      â”‚      â”‚    â”‚      â”‚      â”‚
           stock  time flag ask1  ask2  ...  bid1  bid2 askvol1 askvo2...
```

**S3 ì €ì¥ í˜•ì‹** (JSONL):
```json
{
  "timestamp": "2025-12-15T12:36:07",
  "stock_code": "005930",
  "ask_price_1": 105100.0, "ask_volume_1": 64675.0,
  "ask_price_2": 105200.0, "ask_volume_2": 48203.0,
  ...
  "bid_price_1": 105000.0, "bid_volume_1": 68489.0,
  "bid_price_2": 104900.0, "bid_volume_2": 52301.0,
  ...
  "ask_price_10": 106000.0, "ask_volume_10": 12045.0,
  "bid_price_10": 104100.0, "bid_volume_10": 9834.0
}
```

**Raw Features (40ê°œ)**:
- ask_price_{1-10}: 10ê°œ
- ask_volume_{1-10}: 10ê°œ
- bid_price_{1-10}: 10ê°œ
- bid_volume_{1-10}: 10ê°œ

### 3.3 ë°ì´í„° ë‹¤ìš´ë¡œë“œ

```bash
# S3ì—ì„œ ë¡œì»¬ë¡œ ë‹¤ìš´ë¡œë“œ
aws s3 sync s3://kis-lob-data-20241215/data/ ./data_collection/s3_data/

# ë°ì´í„° í™•ì¸
ls -lh data_collection/s3_data/
# lob_snapshots_20251215_123000.jsonl
# lob_snapshots_20251215_124500.jsonl
# ...
```

---

## 4. íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸

### 4.1 ê°œìš”

**PAPER_DRAFT.md Section 3.3 ê¸°ì¤€**:
- **ì´ 38ê°œ engineered features**
- **6ê°€ì§€ ì¹´í…Œê³ ë¦¬**
- **Raw 40ê°œì™€ ê²°í•©í•˜ì—¬ ì´ 78ê°œ features**

### 4.2 Feature ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸

#### 4.2.1 Price Features (6ê°œ)

**êµ¬í˜„ íŒŒì¼**: `feature_engineering/price_features.py`

```python
def compute_price_features(lob_snapshot: dict) -> dict:
    """
    Price-based features (6 features).

    Returns:
        {
            'mid_price': float,                    # (ask1 + bid1) / 2
            'weighted_mid_price': float,           # VWAP across 10 levels
            'spread_absolute': float,              # ask1 - bid1
            'spread_relative': float,              # (ask1 - bid1) / mid_price
            'log_mid_price': float,                # log(mid_price)
            'mid_price_volatility': float          # 5-event rolling std
        }
    """
```

**ê³„ì‚° ë¡œì§**:
```python
# 1. Mid-price (level 1)
mid_price = (ask_price_1 + bid_price_1) / 2

# 2. Weighted mid-price (VWAP across 10 levels)
total_ask_volume = sum(ask_volume_i for i in 1..10)
total_bid_volume = sum(bid_volume_i for i in 1..10)
vwap_ask = sum(ask_price_i * ask_volume_i) / total_ask_volume
vwap_bid = sum(bid_price_i * bid_volume_i) / total_bid_volume
weighted_mid_price = (vwap_ask + vwap_bid) / 2

# 3. Bid-ask spread (absolute)
spread_absolute = ask_price_1 - bid_price_1

# 4. Bid-ask spread (relative)
spread_relative = spread_absolute / mid_price

# 5. Log mid-price
log_mid_price = log(mid_price)

# 6. Mid-price volatility (5-event rolling std)
# Requires historical buffer of last 5 mid-prices
mid_price_volatility = std(last_5_mid_prices)
```

**ì£¼ì˜ì‚¬í•­**:
- âœ… **Numerical stability**: Division by zero ë°©ì§€ (epsilon = 1e-10)
- âœ… **Causality**: ê³¼ê±° 5ê°œ ì´ë²¤íŠ¸ë§Œ ì‚¬ìš© (ë¯¸ë˜ ì •ë³´ X)
- âœ… **Missing values**: ì²« ì´ë²¤íŠ¸ëŠ” forward-fill

---

#### 4.2.2 Volume Features (8ê°œ)

**êµ¬í˜„ íŒŒì¼**: `feature_engineering/volume_features.py`

```python
def compute_volume_features(lob_snapshot: dict) -> dict:
    """
    Volume-based features (8 features).

    Returns:
        {
            'bid_ask_volume_ratio_1': float,       # bid_vol_1 / ask_vol_1
            'bid_ask_volume_ratio_2': float,
            'bid_ask_volume_ratio_3': float,
            'bid_ask_volume_ratio_4': float,
            'bid_ask_volume_ratio_5': float,
            'cumulative_bid_volume': float,        # sum(bid_vol_1..10)
            'cumulative_ask_volume': float,        # sum(ask_vol_1..10)
            'volume_imbalance_total': float        # (bid_vol - ask_vol) / (bid_vol + ask_vol)
        }
    """
```

**ê³„ì‚° ë¡œì§**:
```python
# 1-5. Bid/Ask volume ratios (levels 1-5)
for i in range(1, 6):
    ratio_i = bid_volume_i / (ask_volume_i + epsilon)

# 6. Total bid volume
cumulative_bid_volume = sum(bid_volume_i for i in 1..10)

# 7. Total ask volume
cumulative_ask_volume = sum(ask_volume_i for i in 1..10)

# 8. Volume imbalance (total)
volume_imbalance_total = (cumulative_bid_volume - cumulative_ask_volume) / \
                         (cumulative_bid_volume + cumulative_ask_volume + epsilon)
```

---

#### 4.2.3 Order Imbalance (OI) Features (6ê°œ)

**êµ¬í˜„ íŒŒì¼**: `feature_engineering/order_imbalance.py`

```python
def compute_order_imbalance_features(lob_snapshot: dict) -> dict:
    """
    Order Imbalance features (6 features).

    Theory: OI measures supply-demand asymmetry.
            Positive OI suggests buying pressure (price likely to increase).

    Returns:
        {
            'oi_level_1': float,                   # (Vbid1 - Vask1) / (Vbid1 + Vask1)
            'oi_level_2': float,
            'oi_level_3': float,
            'oi_total': float,                     # OI across all levels
            'oi_weighted': float,                  # Volume-weighted OI
            'oi_asymmetry': float                  # OI top (1-3) vs deep (4-10)
        }
    """
```

**ê³„ì‚° ë¡œì§**:
```python
# 1-3. OI at levels 1, 2, 3
for i in [1, 2, 3]:
    oi_level_i = (bid_volume_i - ask_volume_i) / (bid_volume_i + ask_volume_i + epsilon)

# 4. OI total (all levels)
total_bid = sum(bid_volume_i for i in 1..10)
total_ask = sum(ask_volume_i for i in 1..10)
oi_total = (total_bid - total_ask) / (total_bid + total_ask + epsilon)

# 5. Weighted OI (closer levels have higher weight)
weights = [1/i for i in range(1, 11)]  # [1.0, 0.5, 0.33, 0.25, ...]
oi_weighted = sum(weights[i] * oi_level_i for i in range(10)) / sum(weights)

# 6. OI asymmetry (top vs deep)
oi_top = sum(bid_volume_i - ask_volume_i for i in 1..3) / \
         sum(bid_volume_i + ask_volume_i for i in 1..3 + epsilon)
oi_deep = sum(bid_volume_i - ask_volume_i for i in 4..10) / \
          sum(bid_volume_i + ask_volume_i for i in 4..10 + epsilon)
oi_asymmetry = oi_top - oi_deep
```

---

#### 4.2.4 Order Flow Imbalance (OFI) Features (6ê°œ)

**êµ¬í˜„ íŒŒì¼**: `feature_engineering/order_flow_imbalance.py`

```python
def compute_order_flow_imbalance_features(
    current_snapshot: dict,
    previous_snapshot: dict
) -> dict:
    """
    Order Flow Imbalance features (6 features).

    Theory: OFI (Cont et al., 2014) measures net order flow changes.
            Strong predictor of price movements.

    OFI formula:
        OFI_bid = Î”V_bid Ã— I[Î”P_bid â‰¥ 0]
        OFI_ask = Î”V_ask Ã— I[Î”P_ask â‰¤ 0]
        OFI_net = OFI_bid - OFI_ask

    Returns:
        {
            'ofi_bid': float,                      # Bid-side order flow
            'ofi_ask': float,                      # Ask-side order flow
            'ofi_net': float,                      # Net order flow (bid - ask)
            'ofi_ratio': float,                    # ofi_bid / (ofi_ask + eps)
            'ofi_cumulative': float,               # Sum of last 5 OFI_net
            'ofi_volatility': float                # Std of last 5 OFI_net
        }
    """
```

**ê³„ì‚° ë¡œì§**:
```python
# Deltas (current - previous)
delta_bid_price_1 = current['bid_price_1'] - previous['bid_price_1']
delta_bid_volume_1 = current['bid_volume_1'] - previous['bid_volume_1']
delta_ask_price_1 = current['ask_price_1'] - previous['ask_price_1']
delta_ask_volume_1 = current['ask_volume_1'] - previous['ask_volume_1']

# 1. OFI bid
# If bid price increased or stayed same, buy market orders absorbed ask liquidity
ofi_bid = delta_bid_volume_1 if delta_bid_price_1 >= 0 else 0

# 2. OFI ask
# If ask price decreased or stayed same, sell market orders absorbed bid liquidity
ofi_ask = delta_ask_volume_1 if delta_ask_price_1 <= 0 else 0

# 3. OFI net
ofi_net = ofi_bid - ofi_ask

# 4. OFI ratio
ofi_ratio = ofi_bid / (ofi_ask + epsilon)

# 5. Cumulative OFI (5-event window)
# Requires buffer of last 5 OFI_net values
ofi_cumulative = sum(last_5_ofi_net)

# 6. OFI volatility (5-event window)
ofi_volatility = std(last_5_ofi_net)
```

**âš ï¸ CRITICAL - Data Leakage Prevention**:
```python
# âœ… CORRECT (uses t and t-1)
ofi_bid[t] = (volume[t] - volume[t-1]) Ã— I[price[t] - price[t-1] â‰¥ 0]

# âŒ WRONG (uses t+1, look-ahead bias!)
ofi_bid[t] = (volume[t+1] - volume[t]) Ã— I[price[t+1] - price[t] â‰¥ 0]
```

---

#### 4.2.5 Depth Features (6ê°œ)

**êµ¬í˜„ íŒŒì¼**: `feature_engineering/depth_features.py`

```python
def compute_depth_features(lob_snapshot: dict) -> dict:
    """
    Depth-based features (6 features).

    Returns:
        {
            'depth_imbalance': float,              # total_bid_volume - total_ask_volume
            'depth_ratio': float,                  # total_bid_volume / total_ask_volume
            'effective_spread': float,             # Volume-weighted spread
            'queue_position_proxy': float,         # Estimated queue position
            'depth_weighted_mid_price': float,     # Depth-weighted price
            'liquidity_concentration': float       # Level 1 volume / total volume
        }
    """
```

**ê³„ì‚° ë¡œì§**:
```python
# 1. Depth imbalance
total_bid_volume = sum(bid_volume_i for i in 1..10)
total_ask_volume = sum(ask_volume_i for i in 1..10)
depth_imbalance = total_bid_volume - total_ask_volume

# 2. Depth ratio
depth_ratio = total_bid_volume / (total_ask_volume + epsilon)

# 3. Effective spread (volume-weighted)
vwap_ask = sum(ask_price_i * ask_volume_i) / total_ask_volume
vwap_bid = sum(bid_price_i * bid_volume_i) / total_bid_volume
effective_spread = vwap_ask - vwap_bid

# 4. Queue position proxy
# Estimate: If we place an order at level 1, how many orders are ahead?
queue_position_proxy = (bid_volume_1 + ask_volume_1) / 2

# 5. Depth-weighted mid-price
# Price weighted by volume at each level
depth_weighted_price = sum((ask_price_i * ask_volume_i + bid_price_i * bid_volume_i)
                          for i in 1..10) / (total_ask_volume + total_bid_volume)

# 6. Liquidity concentration
# What fraction of liquidity is at level 1?
level_1_volume = bid_volume_1 + ask_volume_1
total_volume = total_bid_volume + total_ask_volume
liquidity_concentration = level_1_volume / (total_volume + epsilon)
```

---

#### 4.2.6 Price Impact Features (6ê°œ)

**êµ¬í˜„ íŒŒì¼**: `feature_engineering/price_impact.py`

```python
def compute_price_impact_features(lob_snapshot: dict) -> dict:
    """
    Price Impact features (6 features).

    Theory: Price impact estimates how order flow moves prices (Almgren et al., 2005).

    Returns:
        {
            'market_order_impact_buy': float,      # Price impact of buy market order
            'market_order_impact_sell': float,     # Price impact of sell market order
            'impact_asymmetry': float,             # Buy impact - sell impact
            'resilience_proxy': float,             # Price reversion speed estimate
            'adverse_selection_risk': float,       # Risk of informed trading
            'execution_cost_estimate': float       # Estimated trading cost
        }
    """
```

**ê³„ì‚° ë¡œì§**:
```python
# 1. Market order impact (buy)
# If we submit a market buy order, how much will price move?
# Simplified model: absorb ask liquidity at each level
def estimate_buy_impact(order_size):
    remaining_size = order_size
    total_cost = 0
    for i in range(1, 11):
        if remaining_size <= 0:
            break
        volume_at_level = ask_volume_i
        executed = min(remaining_size, volume_at_level)
        total_cost += executed * ask_price_i
        remaining_size -= executed
    avg_execution_price = total_cost / order_size
    impact = avg_execution_price - ask_price_1  # Price movement
    return impact

# Use standard order size (e.g., 1000 shares)
market_order_impact_buy = estimate_buy_impact(1000)

# 2. Market order impact (sell)
def estimate_sell_impact(order_size):
    remaining_size = order_size
    total_proceeds = 0
    for i in range(1, 11):
        if remaining_size <= 0:
            break
        volume_at_level = bid_volume_i
        executed = min(remaining_size, volume_at_level)
        total_proceeds += executed * bid_price_i
        remaining_size -= executed
    avg_execution_price = total_proceeds / order_size
    impact = bid_price_1 - avg_execution_price  # Price movement
    return impact

market_order_impact_sell = estimate_sell_impact(1000)

# 3. Impact asymmetry
impact_asymmetry = market_order_impact_buy - market_order_impact_sell

# 4. Resilience proxy
# How quickly does price revert after impact?
# Proxy: ratio of level 1 volume to total volume (high = fast reversion)
resilience_proxy = (bid_volume_1 + ask_volume_1) / \
                   (total_bid_volume + total_ask_volume + epsilon)

# 5. Adverse selection risk
# Risk that informed traders are on the other side
# Proxy: spread relative to depth
adverse_selection_risk = (ask_price_1 - bid_price_1) / \
                         ((bid_volume_1 + ask_volume_1) + epsilon)

# 6. Execution cost estimate
# Expected cost to execute a round-trip trade (buy then sell)
execution_cost_estimate = market_order_impact_buy + market_order_impact_sell
```

**Note**: PAPER_DRAFT.md ablation study (Section 4.8)ì—ì„œ **Price Impact featuresê°€ ë‹¨ì¼ ê·¸ë£¹ìœ¼ë¡œ ê°€ì¥ ë†’ì€ ê¸°ì—¬ë„** (+2.41pp)ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.

---

### 4.3 ì „ì²´ íŒŒì´í”„ë¼ì¸

**êµ¬í˜„ íŒŒì¼**: `feature_engineering/pipeline.py`

```python
class FeatureEngineeringPipeline:
    """
    Complete feature engineering pipeline.

    Combines:
    - Raw features (40)
    - Engineered features (38)
    Total: 78 features
    """

    def __init__(self, buffer_size: int = 5):
        """
        Args:
            buffer_size: Number of past events to buffer for temporal features
        """
        self.buffer_size = buffer_size
        self.history_buffer = deque(maxlen=buffer_size)

    def process_snapshot(self, current_snapshot: dict) -> np.ndarray:
        """
        Process a single LOB snapshot into 78 features.

        Args:
            current_snapshot: Dict with keys:
                - timestamp
                - stock_code
                - ask_price_{1-10}, ask_volume_{1-10}
                - bid_price_{1-10}, bid_volume_{1-10}

        Returns:
            feature_vector: np.ndarray of shape (78,)
        """
        # 1. Extract raw features (40)
        raw_features = extract_raw_features(current_snapshot)

        # 2. Compute engineered features (38)
        if len(self.history_buffer) == 0:
            # First event: use current snapshot as previous
            previous_snapshot = current_snapshot
        else:
            previous_snapshot = self.history_buffer[-1]

        price_feats = compute_price_features(
            current_snapshot,
            list(self.history_buffer)
        )  # 6 features

        volume_feats = compute_volume_features(current_snapshot)  # 8 features

        oi_feats = compute_order_imbalance_features(current_snapshot)  # 6 features

        ofi_feats = compute_order_flow_imbalance_features(
            current_snapshot,
            previous_snapshot,
            list(self.history_buffer)
        )  # 6 features

        depth_feats = compute_depth_features(current_snapshot)  # 6 features

        impact_feats = compute_price_impact_features(current_snapshot)  # 6 features

        # 3. Concatenate all features
        feature_vector = np.concatenate([
            raw_features,      # 40
            price_feats,       # 6
            volume_feats,      # 8
            oi_feats,          # 6
            ofi_feats,         # 6
            depth_feats,       # 6
            impact_feats       # 6
        ])  # Total: 78

        # 4. Update history buffer
        self.history_buffer.append(current_snapshot)

        return feature_vector

    def get_feature_names(self) -> List[str]:
        """Return list of all 78 feature names."""
        return [
            # Raw features (40)
            *[f'ask_price_{i}' for i in range(1, 11)],
            *[f'ask_volume_{i}' for i in range(1, 11)],
            *[f'bid_price_{i}' for i in range(1, 11)],
            *[f'bid_volume_{i}' for i in range(1, 11)],

            # Price features (6)
            'mid_price', 'weighted_mid_price', 'spread_absolute',
            'spread_relative', 'log_mid_price', 'mid_price_volatility',

            # Volume features (8)
            *[f'bid_ask_volume_ratio_{i}' for i in range(1, 6)],
            'cumulative_bid_volume', 'cumulative_ask_volume',
            'volume_imbalance_total',

            # OI features (6)
            'oi_level_1', 'oi_level_2', 'oi_level_3',
            'oi_total', 'oi_weighted', 'oi_asymmetry',

            # OFI features (6)
            'ofi_bid', 'ofi_ask', 'ofi_net', 'ofi_ratio',
            'ofi_cumulative', 'ofi_volatility',

            # Depth features (6)
            'depth_imbalance', 'depth_ratio', 'effective_spread',
            'queue_position_proxy', 'depth_weighted_mid_price',
            'liquidity_concentration',

            # Price Impact features (6)
            'market_order_impact_buy', 'market_order_impact_sell',
            'impact_asymmetry', 'resilience_proxy',
            'adverse_selection_risk', 'execution_cost_estimate'
        ]
```

### 4.4 Data Leakage ë°©ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸

```python
# validation/causality_check.py

def verify_no_data_leakage(pipeline: FeatureEngineeringPipeline):
    """
    Comprehensive data leakage verification.

    Based on PAPER_DRAFT.md Section 3.5.3.
    """

    # âœ… Check 1: Temporal causality
    # All features must use only t and t-1 (no future data)
    assert all_features_use_only_past_data(pipeline)

    # âœ… Check 2: OFI causality
    # OFI uses Î”(t) - Î”(t-1), not Î”(t+1)
    assert ofi_uses_correct_deltas(pipeline)

    # âœ… Check 3: Buffer size
    # History buffer only stores past events
    assert pipeline.buffer_size >= 0
    assert len(pipeline.history_buffer) <= pipeline.buffer_size

    # âœ… Check 4: No label information
    # Features cannot use future price labels
    assert features_do_not_use_labels(pipeline)

    print("âœ… All data leakage checks passed!")
```

---

## 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€

### 5.1 CatBoost ì„¤ì •

**êµ¬í˜„ íŒŒì¼**: `model_training/train_catboost.py`

**í•˜ì´í¼íŒŒë¼ë¯¸í„°** (PAPER_DRAFT.md Section 3.4 ê¸°ì¤€):
```python
catboost_params = {
    'iterations': 500,
    'depth': 10,
    'learning_rate': 0.1,
    'loss_function': 'MultiClass',
    'classes_count': 3,
    'eval_metric': 'Accuracy',
    'random_seed': None,  # Will be set per experiment
    'verbose': False,
    'early_stopping_rounds': 50,
    'task_type': 'CPU',  # or 'GPU' if available
    'bootstrap_type': 'Bayesian',  # CatBoost default
}
```

### 5.2 Label ìƒì„±

**PAPER_DRAFT.md Section 3.1**:
- **Prediction horizon**: k=100 events ahead (~5-10 minutes)
- **3-class classification**:
  - Class 0: Price decrease (down)
  - Class 1: Price stationary (no change)
  - Class 2: Price increase (up)

**êµ¬í˜„**:
```python
def generate_labels(lob_snapshots: List[dict], k: int = 100) -> np.ndarray:
    """
    Generate labels for mid-price movement prediction.

    Args:
        lob_snapshots: List of LOB snapshots (chronologically ordered)
        k: Prediction horizon (number of events ahead)

    Returns:
        labels: np.ndarray of shape (n_samples,) with values {0, 1, 2}
    """
    labels = []

    for i in range(len(lob_snapshots) - k):
        current_mid = (lob_snapshots[i]['ask_price_1'] +
                      lob_snapshots[i]['bid_price_1']) / 2
        future_mid = (lob_snapshots[i + k]['ask_price_1'] +
                     lob_snapshots[i + k]['bid_price_1']) / 2

        # Threshold for "stationary" (e.g., Â±0.01%)
        threshold = 0.0001 * current_mid

        if future_mid < current_mid - threshold:
            label = 0  # Down
        elif future_mid > current_mid + threshold:
            label = 2  # Up
        else:
            label = 1  # Stationary

        labels.append(label)

    # Last k samples have no labels (cannot look k events ahead)
    labels.extend([np.nan] * k)

    return np.array(labels)
```

### 5.3 Train/Validation/Test Split

**PAPER_DRAFT.md Section 3.1**:
```
Training:   First 7 days per stock
Validation: Day 8
Test:       Days 9-10
```

**êµ¬í˜„**:
```python
def temporal_train_test_split(
    features: np.ndarray,
    labels: np.ndarray,
    timestamps: np.ndarray,
    stock_codes: np.ndarray
) -> Tuple:
    """
    Split data temporally (no shuffle to prevent look-ahead bias).

    Args:
        features: (n_samples, 78)
        labels: (n_samples,)
        timestamps: (n_samples,) datetime objects
        stock_codes: (n_samples,) stock identifiers

    Returns:
        X_train, X_val, X_test, y_train, y_val, y_test
    """
    # Extract day from timestamp
    days = np.array([ts.day for ts in timestamps])

    # Split per stock
    train_mask = days <= 7
    val_mask = days == 8
    test_mask = days >= 9

    X_train, y_train = features[train_mask], labels[train_mask]
    X_val, y_val = features[val_mask], labels[val_mask]
    X_test, y_test = features[test_mask], labels[test_mask]

    # âœ… Verification: no temporal overlap
    assert max(timestamps[train_mask]) < min(timestamps[test_mask])

    return X_train, X_val, X_test, y_train, y_val, y_test
```

### 5.4 Multi-Seed Validation

**PAPER_DRAFT.md Section 3.5.2**:
- **Seeds**: [42, 123, 456, 789, 1011] (5 seeds, ìµœì†Œ 3 seeds)
- **Metric**: Mean Â± Std
- **Statistical test**: Paired t-test, p-value < 0.05

**êµ¬í˜„**:
```python
def multi_seed_validation(
    X_train, y_train, X_val, y_val, X_test, y_test,
    seeds: List[int] = [42, 123, 456]
) -> dict:
    """
    Train and evaluate model with multiple random seeds.

    Returns:
        {
            'test_accuracies': [acc1, acc2, acc3, ...],
            'mean_accuracy': float,
            'std_accuracy': float,
            'models': [model1, model2, model3, ...]
        }
    """
    test_accuracies = []
    models = []

    for seed in seeds:
        print(f"Training with seed={seed}...")

        # Train CatBoost
        model = CatBoostClassifier(
            **catboost_params,
            random_seed=seed
        )

        model.fit(
            X_train, y_train,
            eval_set=(X_val, y_val),
            verbose=False,
            early_stopping_rounds=50
        )

        # Evaluate on test set
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

        test_accuracies.append(accuracy)
        models.append(model)

        print(f"  Test accuracy: {accuracy:.4f}")

    mean_acc = np.mean(test_accuracies)
    std_acc = np.std(test_accuracies, ddof=1)  # Sample std

    print(f"\nğŸ“Š Results (n={len(seeds)} seeds):")
    print(f"  Mean accuracy: {mean_acc:.4f} Â± {std_acc:.4f}")

    return {
        'test_accuracies': test_accuracies,
        'mean_accuracy': mean_acc,
        'std_accuracy': std_acc,
        'models': models
    }
```

### 5.5 Statistical Significance Testing

**êµ¬í˜„ íŒŒì¼**: `validation/statistical_tests.py`

```python
from scipy import stats

def paired_t_test(
    baseline_accuracies: List[float],
    experiment_accuracies: List[float],
    alpha: float = 0.05
) -> dict:
    """
    Paired t-test comparing two configurations.

    Args:
        baseline_accuracies: List of accuracies for baseline (e.g., Raw only)
        experiment_accuracies: List of accuracies for experiment (e.g., Raw+Engineered)
        alpha: Significance level (default 0.05)

    Returns:
        {
            'mean_diff': float,              # Mean improvement
            't_statistic': float,
            'p_value': float,
            'is_significant': bool,          # p < alpha
            'confidence_interval_95': (lower, upper)
        }
    """
    assert len(baseline_accuracies) == len(experiment_accuracies)

    n = len(baseline_accuracies)
    diffs = np.array(experiment_accuracies) - np.array(baseline_accuracies)

    mean_diff = np.mean(diffs)
    std_diff = np.std(diffs, ddof=1)

    # t-statistic
    t_stat = mean_diff / (std_diff / np.sqrt(n))

    # p-value (two-tailed)
    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-1))

    # 95% confidence interval
    t_critical = stats.t.ppf(1 - alpha/2, df=n-1)
    margin = t_critical * (std_diff / np.sqrt(n))
    ci = (mean_diff - margin, mean_diff + margin)

    return {
        'mean_diff': mean_diff,
        't_statistic': t_stat,
        'p_value': p_value,
        'is_significant': p_value < alpha,
        'confidence_interval_95': ci
    }

# Example usage
baseline = [0.6205, 0.6218, 0.6287, 0.6312, 0.6282]  # Raw only
experiment = [0.6887, 0.6895, 0.6882, 0.6891, 0.6896]  # Raw + Engineered

result = paired_t_test(baseline, experiment)
print(f"Mean improvement: {result['mean_diff']:.4f}")
print(f"t-statistic: {result['t_statistic']:.2f}")
print(f"p-value: {result['p_value']:.6f}")
print(f"Significant: {result['is_significant']} (p < 0.05)")
# Output:
# Mean improvement: +0.0629
# t-statistic: -44.45
# p-value: 0.000002
# Significant: True (p < 0.05)
```

### 5.6 ëª©í‘œ ì„±ëŠ¥

**PAPER_DRAFT.md Section 4.2.2 ê¸°ì¤€**:

| Configuration          | Accuracy (%)    | Std (%) | Î” vs Raw | p-value    | Significant? |
|------------------------|-----------------|---------|----------|------------|--------------|
| Raw baseline (40)      | 68.47 Â± 0.39    | 0.39    | -        | -          | -            |
| Engineered only (38)   | 63.14 Â± 0.21    | 0.21    | -5.33 pp | -          | âŒ Worse     |
| **Raw + Engineered (78)** | **73.43 Â± 0.33** | **0.33** | **+4.96 pp** | **< 0.001** | **âœ… YES** |

**ëª©í‘œ**:
- âœ… **ì •í™•ë„**: 73.43% Â± 0.33% (3-5 seeds)
- âœ… **p-value**: < 0.001 (highly significant)
- âœ… **í‘œì¤€í¸ì°¨**: < 0.4% (robust across seeds)

---

## 6. ì‹¤ì‹œê°„ ì¶”ë¡  ì‹œìŠ¤í…œ

### 6.1 ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Real-time Inference Pipeline                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KIS WebSocket   â”‚  Pipe-delimited LOB data stream
â”‚ (ì¥ì¤‘ ì‹¤ì‹œê°„)    â”‚  0|H0STASP0|001|stock^time^prices^volumes...
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parse & Buffer  â”‚  Convert to dict, maintain history (last 5 events)
â”‚ (buffer.py)     â”‚  {timestamp, stock_code, ask_price_1-10, ...}
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature Computer â”‚  Compute 78 features (40 raw + 38 engineered)
â”‚ (feature_computer.py) â”‚  Uses FeatureEngineeringPipeline
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Predictor â”‚  CatBoost inference
â”‚ (predictor.py)  â”‚  Output: {0: down, 1: stay, 2: up} + probabilities
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Action / Logger â”‚  Log predictions, trigger actions (optional)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 êµ¬í˜„

#### 6.2.1 WebSocket Client

**êµ¬í˜„ íŒŒì¼**: `realtime_inference/websocket_client.py`

```python
import websocket
import json

class KISLOBWebSocketClient:
    """
    KIS API WebSocket client for real-time LOB data.

    Reuses logic from ec2_setup/kis_lob_collector_ec2.py.
    """

    def __init__(self, on_lob_snapshot_callback):
        """
        Args:
            on_lob_snapshot_callback: Callable[(dict), None]
                Called when a new LOB snapshot is received
        """
        self.on_lob_snapshot = on_lob_snapshot_callback
        self.ws = None

    def connect(self, app_key: str, app_secret: str, stock_codes: List[str]):
        """Connect to KIS WebSocket and subscribe to LOB data."""
        # (Implementation similar to kis_lob_collector_ec2.py)
        pass

    def on_message(self, ws, message):
        """Handle incoming WebSocket message."""
        if '|' in message:
            # Pipe-delimited LOB data
            parts = message.split('|')
            if len(parts) >= 4 and parts[1] == 'H0STASP0':
                lob_snapshot = self.parse_pipe_lob_data(parts[3])
                if lob_snapshot:
                    self.on_lob_snapshot(lob_snapshot)

    def parse_pipe_lob_data(self, data_str: str) -> dict:
        """Parse KIS pipe-delimited format into dict."""
        # (Same as kis_lob_collector_ec2.py)
        pass
```

#### 6.2.2 Feature Computer

**êµ¬í˜„ íŒŒì¼**: `realtime_inference/feature_computer.py`

```python
from feature_engineering.pipeline import FeatureEngineeringPipeline

class RealtimeFeatureComputer:
    """
    Real-time feature computation for LOB snapshots.
    """

    def __init__(self, buffer_size: int = 5):
        self.pipeline = FeatureEngineeringPipeline(buffer_size=buffer_size)

    def compute_features(self, lob_snapshot: dict) -> np.ndarray:
        """
        Compute 78 features from LOB snapshot.

        Args:
            lob_snapshot: Dict with ask_price_{1-10}, bid_price_{1-10}, etc.

        Returns:
            features: np.ndarray of shape (78,)
        """
        return self.pipeline.process_snapshot(lob_snapshot)

    def get_feature_names(self) -> List[str]:
        """Return list of 78 feature names."""
        return self.pipeline.get_feature_names()
```

#### 6.2.3 Predictor

**êµ¬í˜„ íŒŒì¼**: `realtime_inference/predictor.py`

```python
from catboost import CatBoostClassifier

class RealtimeLOBPredictor:
    """
    Real-time LOB mid-price movement predictor.
    """

    def __init__(self, model_path: str):
        """
        Args:
            model_path: Path to trained CatBoost model (.cbm file)
        """
        self.model = CatBoostClassifier()
        self.model.load_model(model_path)

    def predict(self, features: np.ndarray) -> dict:
        """
        Predict mid-price movement.

        Args:
            features: np.ndarray of shape (78,) or (n, 78)

        Returns:
            {
                'prediction': int,        # 0: down, 1: stay, 2: up
                'probabilities': [p0, p1, p2],  # Class probabilities
                'confidence': float       # Max probability
            }
        """
        # Ensure 2D shape
        if features.ndim == 1:
            features = features.reshape(1, -1)

        # Predict
        prediction = self.model.predict(features)[0]
        probabilities = self.model.predict_proba(features)[0]
        confidence = max(probabilities)

        return {
            'prediction': int(prediction),
            'probabilities': probabilities.tolist(),
            'confidence': float(confidence)
        }
```

#### 6.2.4 ì „ì²´ í†µí•©

**êµ¬í˜„ íŒŒì¼**: `realtime_inference/main.py`

```python
import time
from realtime_inference.websocket_client import KISLOBWebSocketClient
from realtime_inference.feature_computer import RealtimeFeatureComputer
from realtime_inference.predictor import RealtimeLOBPredictor

class RealtimeLOBInference:
    """
    End-to-end real-time LOB inference system.
    """

    def __init__(self, model_path: str):
        self.feature_computer = RealtimeFeatureComputer(buffer_size=5)
        self.predictor = RealtimeLOBPredictor(model_path)
        self.ws_client = KISLOBWebSocketClient(
            on_lob_snapshot_callback=self.on_new_lob_snapshot
        )

    def on_new_lob_snapshot(self, lob_snapshot: dict):
        """
        Called when new LOB snapshot is received from WebSocket.

        Pipeline:
        1. Compute 78 features
        2. Run model prediction
        3. Log/display result
        """
        stock_code = lob_snapshot['stock_code']
        timestamp = lob_snapshot['timestamp']

        # 1. Compute features
        features = self.feature_computer.compute_features(lob_snapshot)

        # 2. Predict
        result = self.predictor.predict(features)

        # 3. Display
        prediction_label = ['DOWN â¬‡ï¸', 'STAY â¡ï¸', 'UP â¬†ï¸'][result['prediction']]
        confidence = result['confidence']

        print(f"[{timestamp}] {stock_code}: {prediction_label} "
              f"(confidence: {confidence:.2%})")

        # 4. Optional: Trigger actions
        if confidence > 0.80:  # High confidence threshold
            print(f"  ğŸš¨ High confidence signal! Consider action.")

    def start(self, app_key: str, app_secret: str, stock_codes: List[str]):
        """Start real-time inference system."""
        print("ğŸš€ Starting real-time LOB inference system...")
        print(f"ğŸ“Š Monitoring stocks: {stock_codes}")
        print(f"ğŸ¤– Model loaded from: {self.predictor.model}")

        # Connect to WebSocket
        self.ws_client.connect(app_key, app_secret, stock_codes)

        # Keep running
        while True:
            time.sleep(1)

# Usage
if __name__ == '__main__':
    inference_system = RealtimeLOBInference(
        model_path='models/catboost_seed_42.cbm'
    )

    inference_system.start(
        app_key='YOUR_KIS_APP_KEY',
        app_secret='YOUR_KIS_APP_SECRET',
        stock_codes=['005930', '000660', ...]  # 10 stocks
    )
```

---

## 7. ì„±ëŠ¥ ëª©í‘œ ë° ê²€ì¦

### 7.1 ì„±ëŠ¥ ëª©í‘œ (PAPER_DRAFT.md ê¸°ì¤€)

**Primary Metric: Accuracy**

| Experiment              | Target Accuracy | Std   | p-value  | Seeds |
|-------------------------|----------------|-------|----------|-------|
| Raw baseline (40)       | 68.47%         | 0.39% | -        | 3-5   |
| Raw + Engineered (78)   | **73.43%**     | 0.33% | < 0.001  | 3-5   |
| **Improvement**         | **+4.96 pp**   | -     | -        | -     |

**Secondary Metrics (Per-class)**:

| Class | Precision | Recall | F1-Score |
|-------|-----------|--------|----------|
| Down  | 0.70      | 0.72   | 0.71     |
| Stay  | 0.66      | 0.65   | 0.65     |
| Up    | 0.76      | 0.75   | 0.75     |

### 7.2 ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

```
âœ… Statistical Validation
  - [ ] Train with 3-5 random seeds
  - [ ] Report mean Â± std
  - [ ] Run paired t-test vs baseline
  - [ ] Confirm p-value < 0.05
  - [ ] Compute 95% confidence interval

âœ… Data Leakage Verification
  - [ ] Temporal split (train < test)
  - [ ] No future information in features
  - [ ] OFI uses Î”(t-1) not Î”(t+1)
  - [ ] Labels not used in feature computation
  - [ ] Normalization fitted on train only

âœ… Reproducibility
  - [ ] Fixed random seeds
  - [ ] Pin library versions (requirements.txt)
  - [ ] Save all hyperparameters
  - [ ] Log all experiments
  - [ ] Open-source code

âœ… Performance Targets
  - [ ] Accuracy â‰¥ 73.43% (mean across seeds)
  - [ ] Std < 0.4%
  - [ ] Improvement vs baseline > 4.5 pp
  - [ ] p-value < 0.001
```

---

## 8. êµ¬í˜„ ë‹¨ê³„

### Phase 1: Feature Engineering (2ì£¼)

**ëª©í‘œ**: 38ê°œ engineered features êµ¬í˜„ ë° ê²€ì¦

**Task 1.1: Raw Feature ì¶”ì¶œ** (1ì¼)
- [ ] `feature_engineering/raw_features.py` êµ¬í˜„
- [ ] S3 JSONL íŒŒì¼ â†’ 40 raw features ë³€í™˜
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±

**Task 1.2: Engineered Features êµ¬í˜„** (5ì¼)
- [ ] `price_features.py` (6 features) - 1ì¼
- [ ] `volume_features.py` (8 features) - 1ì¼
- [ ] `order_imbalance.py` (6 features) - 1ì¼
- [ ] `order_flow_imbalance.py` (6 features) - 1ì¼
- [ ] `depth_features.py` (6 features) - 0.5ì¼
- [ ] `price_impact.py` (6 features) - 0.5ì¼

**Task 1.3: Pipeline í†µí•©** (2ì¼)
- [ ] `pipeline.py` êµ¬í˜„
- [ ] 78 features ìƒì„± ê²€ì¦
- [ ] Feature ì´ë¦„ ë§¤í•‘ í™•ì¸

**Task 1.4: Data Leakage ê²€ì¦** (2ì¼)
- [ ] `validation/causality_check.py` êµ¬í˜„
- [ ] Temporal causality ê²€ì¦
- [ ] OFI ê³„ì‚° ë¡œì§ ê²€ì¦
- [ ] ëª¨ë“  ì²´í¬ í†µê³¼ í™•ì¸

**Task 1.5: ì„±ëŠ¥ ìµœì í™”** (2ì¼)
- [ ] Numba JIT ì ìš© (optional)
- [ ] Pre-allocated arrays
- [ ] Batch processing
- [ ] ì²˜ë¦¬ ì†ë„ ì¸¡ì • (target: > 100 snapshots/sec)

**Deliverable**:
- âœ… 78 features ìƒì„± íŒŒì´í”„ë¼ì¸
- âœ… ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (coverage > 90%)
- âœ… Data leakage ê²€ì¦ ì™„ë£Œ
- âœ… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ

---

### Phase 2: Model Training (1ì£¼)

**ëª©í‘œ**: CatBoost í•™ìŠµ ë° 73.43% ëª©í‘œ ë‹¬ì„±

**Task 2.1: Label ìƒì„±** (1ì¼)
- [ ] k=100 prediction horizon labels
- [ ] 3-class ë¶„ë¥˜ (down/stay/up)
- [ ] Label distribution í™•ì¸

**Task 2.2: Train/Test Split** (0.5ì¼)
- [ ] Temporal split (7 days train, 1 day val, 2 days test)
- [ ] ëˆ„ìˆ˜ ê²€ì¦
- [ ] ë°ì´í„° ë¶„í¬ í™•ì¸

**Task 2.3: Single-Seed Baseline** (1ì¼)
- [ ] Raw only (40 features) í•™ìŠµ
- [ ] Raw + Engineered (78 features) í•™ìŠµ
- [ ] ì„±ëŠ¥ ë¹„êµ

**Task 2.4: Multi-Seed Validation** (2ì¼)
- [ ] 3-5 seeds í•™ìŠµ
- [ ] Mean Â± Std ê³„ì‚°
- [ ] Paired t-test
- [ ] ëª©í‘œ ë‹¬ì„± í™•ì¸ (73.43% Â± 0.33%)

**Task 2.5: Hyperparameter Tuning** (1ì¼, optional)
- [ ] Grid search (iterations, depth, lr)
- [ ] 3-fold CV on validation set
- [ ] Best config ì„ ì •

**Task 2.6: Model Saving** (0.5ì¼)
- [ ] Save best models (.cbm files)
- [ ] Feature importance ë¶„ì„
- [ ] ê²°ê³¼ ì‹œê°í™”

**Deliverable**:
- âœ… Trained CatBoost models (3-5 seeds)
- âœ… ì„±ëŠ¥ ë³´ê³ ì„œ (accuracy, p-value, CI)
- âœ… Feature importance ranking
- âœ… Confusion matrix

---

### Phase 3: Real-time Inference (1ì£¼)

**ëª©í‘œ**: ì‹¤ì‹œê°„ ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬ì¶•

**Task 3.1: WebSocket Client** (2ì¼)
- [ ] `websocket_client.py` êµ¬í˜„
- [ ] KIS API ì—°ë™
- [ ] íŒŒì´í”„ êµ¬ë¶„ í˜•ì‹ íŒŒì‹±

**Task 3.2: Feature Computer** (1ì¼)
- [ ] `feature_computer.py` êµ¬í˜„
- [ ] Real-time buffer ê´€ë¦¬
- [ ] 78 features ê³„ì‚°

**Task 3.3: Predictor** (1ì¼)
- [ ] `predictor.py` êµ¬í˜„
- [ ] CatBoost model loading
- [ ] Prediction output formatting

**Task 3.4: í†µí•© ë° í…ŒìŠ¤íŠ¸** (2ì¼)
- [ ] `main.py` êµ¬í˜„
- [ ] End-to-end í…ŒìŠ¤íŠ¸
- [ ] Latency ì¸¡ì • (target: < 100ms)

**Task 3.5: ë°°í¬ ë° ëª¨ë‹ˆí„°ë§** (1ì¼)
- [ ] EC2 ë°°í¬ (optional)
- [ ] Logging ì„¤ì •
- [ ] ì•Œë¦¼ ì‹œìŠ¤í…œ (optional)

**Deliverable**:
- âœ… ì‹¤ì‹œê°„ ì¶”ë¡  ì‹œìŠ¤í…œ
- âœ… Latency < 100ms
- âœ… ì¥ì¤‘ ì‹¤ì‹œê°„ ë™ì‘ í™•ì¸

---

### Phase 4: Experiments (ì„ íƒ, 1ì£¼)

**ëª©í‘œ**: ì¶”ê°€ ì‹¤í—˜ (PAPER_DRAFT.md ì¬í˜„)

**Task 4.1: Ablation Study** (2ì¼)
- [ ] Feature groupë³„ ê¸°ì—¬ë„ ë¶„ì„
- [ ] Price Impact vs OI vs OFI ë¹„êµ
- [ ] ê²°ê³¼ ë³´ê³ ì„œ

**Task 4.2: Random Baseline** (1ì¼)
- [ ] 38 random features ìƒì„±
- [ ] Raw + Random (78) vs Raw + Engineered (78)
- [ ] Domain knowledge ê¸°ì—¬ë„ ë¶„ë¦¬

**Task 4.3: TransLOB Comparison** (2ì¼, optional)
- [ ] TransLOB êµ¬í˜„
- [ ] Raw vs Raw+Engineered ë¹„êµ
- [ ] CatBoost vs TransLOB

**Task 4.4: Cross-Stock Analysis** (1ì¼)
- [ ] ì¢…ëª©ë³„ ì„±ëŠ¥ ë¶„ì„
- [ ] Feature importance ë¹„êµ
- [ ] Generalization í‰ê°€

**Deliverable**:
- âœ… Ablation study ë³´ê³ ì„œ
- âœ… Random baseline ë¹„êµ
- âœ… TransLOB ë¹„êµ (optional)
- âœ… Cross-stock analysis

---

## 9. ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘

### 9.1 ë°ì´í„° í’ˆì§ˆ ë¦¬ìŠ¤í¬

**ë¦¬ìŠ¤í¬**: S3 ë°ì´í„°ì— missing values, outliers ì¡´ì¬ ê°€ëŠ¥

**ëŒ€ì‘**:
1. ë°ì´í„° ì „ì²˜ë¦¬ ì‹œ anomaly detection
2. Missing values forward-fill
3. Outlier filtering (IQR method)

### 9.2 ì„±ëŠ¥ ëª©í‘œ ë¯¸ë‹¬ ë¦¬ìŠ¤í¬

**ë¦¬ìŠ¤í¬**: 73.43% ëª©í‘œ ë¯¸ë‹¬ì„±

**ëŒ€ì‘**:
1. Hyperparameter tuning (depth, iterations, lr)
2. Feature selection (ìƒìœ„ 50ê°œ featuresë§Œ ì‚¬ìš©)
3. Ensemble methods (ì—¬ëŸ¬ ëª¨ë¸ ê²°í•©)
4. í•œêµ­ ë°ì´í„° íŠ¹ì„± ë°˜ì˜ (FI-2010ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)

**Fallback ëª©í‘œ**:
- Minimum acceptable: 70% accuracy (Raw baseline ëŒ€ë¹„ +1.5 pp)
- í†µê³„ì  ìœ ì˜ì„± (p < 0.05) ìœ ì§€

### 9.3 ì‹¤ì‹œê°„ Latency ë¦¬ìŠ¤í¬

**ë¦¬ìŠ¤í¬**: Feature ê³„ì‚° ì‹œê°„ > 100ms

**ëŒ€ì‘**:
1. Numba JIT ì»´íŒŒì¼
2. Pre-computed features (ì¼ë¶€)
3. Caching (buffer ì¬ì‚¬ìš©)
4. C++ extension (ìµœí›„ ìˆ˜ë‹¨)

### 9.4 ë°ì´í„° ëˆ„ìˆ˜ ë¦¬ìŠ¤í¬

**ë¦¬ìŠ¤í¬**: ì˜ë„ì¹˜ ì•Šì€ look-ahead bias

**ëŒ€ì‘**:
1. Comprehensive causality check
2. Code review (2ëª… ì´ìƒ)
3. Temporal split ì¬ê²€ì¦
4. Feature computation ë¡œì§ ê°ì‚¬

---

## 10. ì°¸ê³  ìë£Œ

### 10.1 í•µì‹¬ ë…¼ë¬¸

1. **Cont et al. (2014)**: OFI ì´ë¡ 
   "The price impact of order book events"

2. **PAPER_DRAFT.md**: ì´ í”„ë¡œì íŠ¸ì˜ ê¸°ì¤€ ë¬¸ì„œ
   - 38 engineered features ì •ì˜
   - 73.43% ì„±ëŠ¥ ëª©í‘œ
   - Statistical validation ë°©ë²•ë¡ 

3. **Almgren et al. (2005)**: Price impact ì´ë¡ 
   "Direct estimation of equity market impact"

### 10.2 ì½”ë“œ ì°¸ê³ 

- **FI-2010 Dataset**: https://etsin.fairdata.fi/dataset/73eb48d7-4dbc-4a10-a52a-da745b47a649
- **CatBoost Documentation**: https://catboost.ai/
- **KIS API Documentation**: (í•œêµ­íˆ¬ìì¦ê¶Œ API ë¬¸ì„œ)

---

## ìš”ì•½

**ì´ ëª…ì„¸ì„œëŠ” PAPER_DRAFT.mdë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤:**

âœ… **Raw 40 + Engineered 38 = 78 features**
âœ… **CatBoost í•™ìŠµ, ëª©í‘œ 73.43% Â± 0.33%**
âœ… **Statistical validation (3-5 seeds, p < 0.001)**
âœ… **Data leakage ë°©ì§€ (temporal split, causality check)**
âœ… **ì‹¤ì‹œê°„ ì¶”ë¡  ì‹œìŠ¤í…œ (WebSocket â†’ Features â†’ Prediction)**

**ë‹¤ìŒ ë‹¨ê³„**:
1. Phase 1: Feature Engineering êµ¬í˜„ ì‹œì‘
2. S3 ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬
3. 38 engineered features ê³„ì‚° ê²€ì¦
4. Data leakage check í†µê³¼ í™•ì¸

**ì§ˆë¬¸ì´ë‚˜ ìˆ˜ì • ì‚¬í•­ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ì£¼ì„¸ìš”!**
